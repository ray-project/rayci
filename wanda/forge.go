package wanda

import (
	"fmt"
	"log"
	"os"
	"path/filepath"
	"runtime"
	"strings"

	"github.com/google/go-containerregistry/pkg/authn"
	cranename "github.com/google/go-containerregistry/pkg/name"
	crane "github.com/google/go-containerregistry/pkg/v1"
	"github.com/google/go-containerregistry/pkg/v1/remote"
)

// Build builds a container image from the given specification file, and builds
// all its dependencies in topological order.
// In RayCI mode, dependencies are assumed built by prior pipeline steps; only
// the root is built.
func Build(specFile string, config *ForgeConfig) error {
	if config == nil {
		config = &ForgeConfig{}
	}

	graph, err := buildDepGraph(specFile, os.LookupEnv, config.NamePrefix)
	if err != nil {
		return fmt.Errorf("build dep graph: %w", err)
	}

	if err := graph.validateDeps(); err != nil {
		return fmt.Errorf("validate deps: %w", err)
	}

	forge, err := NewForge(config)
	if err != nil {
		return fmt.Errorf("make forge: %w", err)
	}

	// In RayCI mode, only build the root (deps built by prior pipeline steps).
	order := graph.Order
	if config.RayCI {
		order = []string{graph.Root}
	}

	for _, name := range order {
		rs := graph.Specs[name]

		log.Printf("building %s (from %s)", name, rs.Path)

		if err := forge.Build(rs.Spec); err != nil {
			return fmt.Errorf("build %s: %w", name, err)
		}
	}

	return nil
}

// Forge is a forge to build container images.
type Forge struct {
	config *ForgeConfig

	workDir string

	remoteOpts []remote.Option

	cacheHitCount int

	docker *dockerCmd
}

// NewForge creates a new forge with the given configuration.
func NewForge(config *ForgeConfig) (*Forge, error) {
	absWorkDir, err := filepath.Abs(filepath.FromSlash(config.WorkDir))
	if err != nil {
		return nil, fmt.Errorf("abs path for work dir: %w", err)
	}

	f := &Forge{
		config:  config,
		workDir: absWorkDir,
		remoteOpts: []remote.Option{
			remote.WithAuthFromKeychain(authn.DefaultKeychain),
			remote.WithPlatform(crane.Platform{
				OS:           runtime.GOOS,
				Architecture: runtime.GOARCH,
			}),
		},
	}
	f.docker = f.newDockerCmd()

	return f, nil
}

func (f *Forge) cacheHit() int { return f.cacheHitCount }

func (f *Forge) addSrcFile(ts *tarStream, src string) {
	ts.addFile(src, nil, filepath.Join(f.workDir, filepath.FromSlash(src)))
}

func (f *Forge) isRemote() bool             { return f.config.isRemote() }
func (f *Forge) workTag(name string) string { return f.config.workTag(name) }
func (f *Forge) cacheTag(digest string) string {
	return f.config.cacheTag(digest)
}

func (f *Forge) newDockerCmd() *dockerCmd {
	return newDockerCmd(&dockerCmdConfig{
		bin:             f.config.DockerBin,
		useLegacyEngine: runtime.GOOS == "windows",
	})
}

func (f *Forge) resolveBases(froms []string) (map[string]*imageSource, error) {
	m := make(map[string]*imageSource)
	namePrefix := f.config.NamePrefix

	for _, from := range froms {
		if strings.HasPrefix(from, "@") { // A local image.
			name := strings.TrimPrefix(from, "@")
			src, err := resolveDockerImage(f.docker, from, name)
			if err != nil {
				return nil, fmt.Errorf("resolve local image %s: %w", from, err)
			}
			m[from] = src
			continue
		}

		if namePrefix != "" && strings.HasPrefix(from, namePrefix) {
			if !f.isRemote() {
				// Treat it as a local image.
				src, err := resolveDockerImage(f.docker, from, from)
				if err != nil {
					return nil, fmt.Errorf(
						"resolve prefixed local image %s: %w", from, err,
					)
				}
				m[from] = src
				continue
			}

			// An image in the work namespace. It is generated by a previous
			// job, and we need to pull it from the work repo.
			fromName := strings.TrimPrefix(from, f.config.NamePrefix)
			workTag := f.workTag(fromName)

			src, err := resolveRemoteImage(from, workTag, f.remoteOpts...)
			if err != nil {
				return nil, fmt.Errorf(
					"resolve remote work image %s: %w", from, err,
				)
			}
			m[from] = src
			continue
		}

		// A normal remote image that we need to pull from the network.
		src, err := resolveRemoteImage(from, from, f.remoteOpts...)
		if err != nil {
			return nil, fmt.Errorf("resolve remote image %s: %w", from, err)
		}
		m[from] = src
	}
	return m, nil
}

// Build builds a container image from the given specification.
func (f *Forge) Build(spec *Spec) error {
	// Prepare the tar stream.
	ts := newTarStream()

	files, err := listSrcFiles(f.workDir, spec.Srcs, spec.Dockerfile)
	if err != nil {
		return fmt.Errorf("list src files: %w", err)
	}
	for _, file := range files {
		f.addSrcFile(ts, file)
	}

	in := newBuildInput(ts, spec.BuildArgs)

	froms, err := f.resolveBases(spec.Froms)
	if err != nil {
		return fmt.Errorf("resolve bases: %w", err)
	}
	in.froms = froms

	inputCore, err := in.makeCore(spec.Dockerfile)
	if err != nil {
		return fmt.Errorf("make build input core: %w", err)
	}
	inputCore.Epoch = f.config.Epoch

	caching := !spec.DisableCaching

	inputDigest, err := inputCore.digest()
	if err != nil {
		return fmt.Errorf("compute build input digest: %w", err)
	}
	log.Println("build input digest:", inputDigest)

	cacheTag := f.cacheTag(inputDigest)
	workTag := f.workTag(spec.Name)

	// Add all the tags.

	// Work tag is the tag we use to save the image in the work repo.
	in.addTag(workTag)
	in.addTag(cacheTag)

	// When running on rayCI, we only need the workTag and the cacheTag.
	// Otherwise, add extra tags.
	if !f.config.RayCI {
		// Name tag is the tag we use to reference the image locally.
		// It is also what can be referenced by following steps.
		if f.config.NamePrefix != "" {
			nameTag := f.config.NamePrefix + spec.Name
			in.addTag(nameTag)
		}
		for _, tag := range spec.Tags { // And add extra tags.
			in.addTag(tag)
		}
	}

	if caching && !f.config.Rebuild {
		if f.isRemote() {
			ct, err := cranename.NewTag(cacheTag)
			if err != nil {
				return fmt.Errorf("parse cache tag %q: %w", cacheTag, err)
			}
			wt, err := cranename.NewTag(workTag)
			if err != nil {
				return fmt.Errorf("parse work tag %q: %w", workTag, err)
			}

			desc, err := remote.Get(ct, f.remoteOpts...)
			if err != nil {
				log.Printf("cache image miss: %v", err)
			} else {
				log.Printf("cache hit: %s", desc.Digest)
				f.cacheHitCount++

				log.Printf("tag output as %s", workTag)
				if err := remote.Tag(wt, desc, f.remoteOpts...); err != nil {
					return fmt.Errorf("tag cache image: %w", err)
				}

				return nil // and we are done.
			}
		} else {
			info, err := f.docker.inspectImage(cacheTag)
			if err != nil {
				return fmt.Errorf("check cache image: %w", err)
			}
			if info != nil {
				log.Printf("cache hit: %s", info.ID)
				f.cacheHitCount++

				for _, tag := range in.tagList() {
					log.Printf("tag output as %s", tag)
					if tag != cacheTag {
						if err := f.docker.tag(cacheTag, tag); err != nil {
							return fmt.Errorf("tag cache image: %w", err)
						}
					}
				}
				return nil // and we are done.
			}
		}
	}

	inputHints := newBuildInputHints(spec.BuildHintArgs)

	// Now we can build the image.
	// Always use a new dockerCmd so that it can run in its own environment.
	d := f.newDockerCmd()
	d.setWorkDir(f.workDir)

	if err := d.build(in, inputCore, inputHints); err != nil {
		return fmt.Errorf("build docker: %w", err)
	}

	// Push the image to the work repo with workTag and cacheTag if needed.
	if f.isRemote() {
		if err := d.run("push", workTag); err != nil {
			return fmt.Errorf("push docker: %w", err)
		}

		// Save cache result too.
		if caching && !f.config.ReadOnlyCache {
			if err := d.run("push", cacheTag); err != nil {
				return fmt.Errorf("push cache: %w", err)
			}
		}
	}

	return nil
}
