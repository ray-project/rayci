package wanda

import (
	"fmt"
	"log"
	"os"
	"path/filepath"
	"runtime"
	"strings"

	"github.com/google/go-containerregistry/pkg/authn"
	cranename "github.com/google/go-containerregistry/pkg/name"
	crane "github.com/google/go-containerregistry/pkg/v1"
	"github.com/google/go-containerregistry/pkg/v1/remote"
)

// Build builds a container image from the given specification file.
func Build(specFile string, config *ForgeConfig) error {
	if config == nil {
		config = &ForgeConfig{}
	}

	spec, err := parseSpecFile(specFile)
	if err != nil {
		return fmt.Errorf("parse spec file: %w", err)
	}

	// Expand env variable.
	spec = spec.expandVar(os.LookupEnv)

	forge, err := NewForge(config)
	if err != nil {
		return fmt.Errorf("make forge: %w", err)
	}
	return forge.Build(spec)
}

// ForgeConfig is a configuration for a forge to build container images.
type ForgeConfig struct {
	WorkDir    string
	DockerBin  string
	WorkRepo   string
	NamePrefix string
	BuildID    string
	Epoch      string

	RayCI   bool
	Remote  bool
	Rebuild bool

	ReadOnlyCache bool
}

// Forge is a forge to build container images.
type Forge struct {
	config *ForgeConfig

	workDir string

	remoteOpts []remote.Option

	cacheHitCount int

	docker *dockerCmd
}

// NewForge creates a new forge with the given configuration.
func NewForge(config *ForgeConfig) (*Forge, error) {
	absWorkDir, err := filepath.Abs(config.WorkDir)
	if err != nil {
		return nil, fmt.Errorf("abs path for work dir: %w", err)
	}

	f := &Forge{
		config:  config,
		workDir: absWorkDir,
		remoteOpts: []remote.Option{
			remote.WithAuthFromKeychain(authn.DefaultKeychain),
			remote.WithPlatform(crane.Platform{
				OS:           runtime.GOOS,
				Architecture: runtime.GOARCH,
			}),
		},
	}
	f.docker = f.newDockerCmd()

	return f, nil
}

func (f *Forge) cacheHit() int { return f.cacheHitCount }

func (f *Forge) addSrcFile(ts *tarStream, src string) {
	ts.addFile(src, nil, filepath.Join(f.workDir, src))
}

func (f *Forge) workRepo() string {
	if f.config.WorkRepo != "" {
		return f.config.WorkRepo
	}
	return "localhost:5000/rayci-work"
}

func (f *Forge) workTag(name string) string {
	workRepo := f.workRepo()
	if f.config.BuildID != "" {
		return fmt.Sprintf("%s:%s-%s", workRepo, f.config.BuildID, name)
	}
	return fmt.Sprintf("%s:%s", workRepo, name)
}

func (f *Forge) cacheTag(inputDigest string) string {
	if _, d, ok := strings.Cut(inputDigest, ":"); ok {
		inputDigest = d
	}
	return fmt.Sprintf("%s:z-%s", f.workRepo(), inputDigest)
}

func (f *Forge) newDockerCmd() *dockerCmd {
	return newDockerCmd(&dockerCmdConfig{
		bin:             f.config.DockerBin,
		useLegacyEngine: runtime.GOOS == "windows",
	})
}

func (f *Forge) resolveLocalImage(name, ref string) (*imageSource, error) {
	if f.config.Remote {
		return resolveLocalImage(name, ref)
	}
	return resolveDockerImage(f.docker, name, ref)
}

func (f *Forge) resolveBases(froms []string) (map[string]*imageSource, error) {
	m := make(map[string]*imageSource)
	namePrefix := f.config.NamePrefix

	for _, from := range froms {
		if strings.HasPrefix(from, "@") { // A local image.
			name := strings.TrimPrefix(from, "@")
			src, err := f.resolveLocalImage(from, name)
			if err != nil {
				return nil, fmt.Errorf("resolve local image %s: %w", from, err)
			}
			m[from] = src
			continue
		}

		if namePrefix != "" && strings.HasPrefix(from, namePrefix) {
			if f.config.WorkRepo == "" {
				// Treat it as a local image.
				src, err := f.resolveLocalImage(from, from)
				if err != nil {
					return nil, fmt.Errorf(
						"resolve prefixed local image %s: %w", from, err,
					)
				}
				m[from] = src
				continue
			}

			// An image in the work namespace. It is generated by a previous
			// job, and we need to pull it from the work repo.
			fromName := strings.TrimPrefix(from, f.config.NamePrefix)
			workTag := f.workTag(fromName)

			src, err := resolveRemoteImage(from, workTag, f.remoteOpts...)
			if err != nil {
				return nil, fmt.Errorf(
					"resolve remote work image %s: %w", from, err,
				)
			}
			m[from] = src
			continue
		}

		// A normal remote image that we need to pull from the network.
		src, err := resolveRemoteImage(from, from, f.remoteOpts...)
		if err != nil {
			return nil, fmt.Errorf("resolve remote image %s: %w", from, err)
		}
		m[from] = src
	}
	return m, nil
}

// Build builds a container image from the given specification.
func (f *Forge) Build(spec *Spec) error {
	// Prepare the tar stream.
	var ts *tarStream
	if !spec.CopyEverything {
		ts = newTarStream()
		f.addSrcFile(ts, spec.Dockerfile)
		for _, src := range spec.Srcs {
			f.addSrcFile(ts, src)
		}
	}

	in := newBuildInput(ts, spec.BuildArgs)

	froms, err := f.resolveBases(spec.Froms)
	if err != nil {
		return fmt.Errorf("resolve bases: %w", err)
	}
	in.froms = froms

	inputCore, err := in.makeCore(spec.Dockerfile)
	if err != nil {
		return fmt.Errorf("make build input core: %w", err)
	}
	inputCore.Epoch = f.config.Epoch

	inputDigest, err := inputCore.digest()
	if err != nil {
		return fmt.Errorf("compute build input digest: %w", err)
	}
	log.Println("build input digest:", inputDigest)

	cacheTag := f.cacheTag(inputDigest)
	workTag := f.workTag(spec.Name)
	cachable := !spec.CopyEverything

	// Add all the tags.

	// Work tag is the tag we use to save the image in the work repo.
	in.addTag(workTag)
	if cachable {
		in.addTag(cacheTag)
	}

	// When running on rayCI, we only need the workTag and the cacheTag.
	// Otherwise, add extra tags.
	if !f.config.RayCI {
		// Name tag is the tag we use to reference the image locally.
		// It is also what can be referenced by following steps.
		if f.config.NamePrefix != "" {
			nameTag := f.config.NamePrefix + spec.Name
			in.addTag(nameTag)
		}
		// And add any extra tags.
		for _, tag := range spec.Tags {
			in.addTag(tag)
		}
	}

	if cachable && !f.config.Rebuild {
		if f.config.Remote {
			ct, err := cranename.NewTag(cacheTag)
			if err != nil {
				return fmt.Errorf("parse cache tag %q: %w", cacheTag, err)
			}

			wt, err := cranename.NewTag(workTag)
			if err != nil {
				return fmt.Errorf("parse work tag %q: %w", workTag, err)
			}

			desc, err := remote.Get(ct, f.remoteOpts...)
			if err != nil {
				log.Printf("Cache image miss: %v", err)
			} else {
				// Cache hit!
				log.Printf("cache hit: %s", desc.Digest)
				f.cacheHitCount++

				log.Printf("tag output as %s", workTag)
				if err := remote.Tag(wt, desc, f.remoteOpts...); err != nil {
					return fmt.Errorf("tag cache image: %w", err)
				}

				return nil // and we are done.
			}
		} else {
			info, err := f.docker.inspectImage(cacheTag)
			if err != nil {
				return fmt.Errorf("check cache image: %w", err)
			}
			if info != nil {
				// Cache hit!
				log.Printf("cache hit: %s", info.Id)
				f.cacheHitCount++

				for _, tag := range in.tagList() {
					log.Printf("tag output as %s", tag)
					if tag == cacheTag {
						continue
					}
					if err := f.docker.tag(cacheTag, tag); err != nil {
						return fmt.Errorf("tag cache image: %w", err)
					}
				}
				return nil // and we are done.
			}
		}
	}

	// Now we can build the image.
	// Always use a new dockerCmd so that it can run in its own environment.
	d := f.newDockerCmd()
	d.setWorkDir(f.workDir)

	if err := d.build(in, inputCore); err != nil {
		return fmt.Errorf("build docker: %w", err)
	}

	// Push the image to the work repo.
	if f.config.Remote {
		if err := d.run("push", workTag); err != nil {
			return fmt.Errorf("push docker: %w", err)
		}

		// Save cache result too.
		if cachable && !f.config.ReadOnlyCache {
			if err := d.run("push", cacheTag); err != nil {
				return fmt.Errorf("push cache: %w", err)
			}
		}
	}

	return nil
}
